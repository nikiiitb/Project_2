{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-14 17:56:14.064439: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-14 17:56:14.862496: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/user/miniconda3/envs/python38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from transformers import AutoModel , AutoImageProcessor , AutoTokenizer \n",
    "from datasets import load_dataset , Dataset\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_img_address(input):\n",
    "    IMAGE_DIR = \"./train2014_3d\"\n",
    "    input[\"image_id\"] = f\"{IMAGE_DIR}/{input['image_id']}\"\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 1112.13 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'question_type', 'question_id', 'image_id', 'answer_type', 'label'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SAMPLES = 10\n",
    "\n",
    "df_acc = pd.read_pickle(\"./vqa_v2_acc.pkl\")\n",
    "dataset = Dataset.from_pandas(df_acc)\n",
    "dataset = dataset.remove_columns(['__index_level_0__'])\n",
    "dataset = dataset.select(range(0,SAMPLES))\n",
    "dataset = dataset.map(training_img_address)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(640, 480)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Image.open(dataset[0][\"image_id\"]).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10, 480, 640, 3), (10,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_img = np.array([np.array(Image.open(i).resize((640 , 480))) for i in dataset[\"image_id\"]])\n",
    "X_text = np.array(dataset[\"question\"])\n",
    "X_img.shape , X_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "labels = [item['ids'] for item in dataset['label']]\n",
    "flattened_labels = list(itertools.chain(*labels))\n",
    "unique_labels = list(set(flattened_labels))\n",
    "\n",
    "label2id = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'red and white': 0,\n",
       " 'red': 1,\n",
       " 'flying disc': 2,\n",
       " 'skiing': 3,\n",
       " 'black': 4,\n",
       " 'red & white': 5,\n",
       " 'no': 6,\n",
       " 'pitcher': 7,\n",
       " 'mesh': 8,\n",
       " 'frisbie': 9,\n",
       " 'white': 10,\n",
       " 'white frisbee': 11,\n",
       " 'net': 12,\n",
       " 'frisbee': 13,\n",
       " 'catcher': 14,\n",
       " 'yes': 15,\n",
       " 'netting': 16,\n",
       " 'orange': 17}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 2096.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Classes :  18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def replace_ids(inputs):\n",
    "    '''Converting everything to one-hot-encoding'''\n",
    "    h = [0 for i in id2label]\n",
    "    for i in range(len(inputs[\"label\"][\"ids\"])):\n",
    "        t = inputs[\"label\"][\"ids\"][i]\n",
    "        w = inputs[\"label\"][\"weights\"][i]\n",
    "        if w > 0.5: w = 1\n",
    "        else: w = 0.3\n",
    "        # print(t , w)\n",
    "        # print(label2id.get(t , 0))\n",
    "        h[label2id.get(t , 0)] = w\n",
    "    inputs[\"label\"] = h\n",
    "    return inputs\n",
    "\n",
    "\n",
    "flat_dataset = dataset.map(replace_ids)\n",
    "# flat_dataset = dataset.flatten()\n",
    "NUM_CLASSES = np.array(flat_dataset[\"label\"]).shape[1]\n",
    "print(\"Number of Classes : \" , NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing BERT and ViT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/python38/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "DEVICE = \"cuda\"\n",
    "\n",
    "vit_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "# model = AutoModel.from_pretrained(\"google/vit-base-patch16-224\").to(DEVICE)\n",
    "vit_model = AutoModel.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/python38/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer , BertModel\n",
    "\n",
    "bert_processor = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip = vit_processor(X_img , return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['last_hidden_state', 'pooler_output'])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# outp = vit_model(**ip)\n",
    "outp = vit_model(ip[\"pixel_values\"])\n",
    "outp.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Concatentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 1536])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = outp.pooler_output\n",
    "\n",
    "\n",
    "batch_sentences = dataset[\"question\"]\n",
    "encoded_input = bert_processor(batch_sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "input_ids = encoded_input[\"input_ids\"]\n",
    "attention_mask = encoded_input[\"attention_mask\"]\n",
    "y = bert_model(input_ids , attention_mask=attention_mask).pooler_output\n",
    "\n",
    "x.shape , y.shape\n",
    "\n",
    "torch.cat([x,y] , axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/python38/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['vit.pooler.dense.bias', 'vit.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "class CustomVITModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomVITModel, self).__init__()\n",
    "        self.vit = AutoModel.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        ### New layers:\n",
    "        self.linear1 = nn.Linear(768 * 2, 256)\n",
    "        self.linear2 = nn.Linear(256, NUM_CLASSES) ## 3 is the number of classes in this example\n",
    "        self.sigmoid = nn.Softmax()\n",
    "\n",
    "    def forward(self, pixel_values , input_ids , attention_mask):\n",
    "        vit_outp = self.vit(pixel_values)\n",
    "        vit_last_hidden_state, vit_pooled_output = vit_outp.last_hidden_state , vit_outp.pooler_output\n",
    "        \n",
    "        bert_outp = self.bert(input_ids , attention_mask=attention_mask)\n",
    "        bert_last_hidden_state, bert_pooled_output = bert_outp.last_hidden_state , bert_outp.pooler_output\n",
    "        \n",
    "        # last_hidden_state = last_hidden_state.cuda()\n",
    "        # pooled_output = pooled_output.cuda()\n",
    "\n",
    "        # sequence_output has the following shape: (batch_size, sequence_length, 768)\n",
    "        \n",
    "        combined_input = torch.cat([vit_pooled_output,bert_pooled_output] , axis=1)\n",
    "        \n",
    "        # linear1_output = self.linear1(last_hidden_state[:,0,:].view(-1,768)) ## extract the 1st token's embeddings\n",
    "        linear1_output = self.linear1(combined_input)\n",
    "\n",
    "        linear2_output = self.linear2(linear1_output)\n",
    "        \n",
    "        pred = linear2_output\n",
    "        # pred = self.sigmoid(linear2_output)\n",
    "\n",
    "        return pred\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-italian-xxl-cased\") \n",
    "model_custom = CustomVITModel() # You can pass the parameters if required to have more flexible model\n",
    "# model_custom.to(\"cuda\") ## can be gpu\n",
    "criterion = nn.CrossEntropyLoss() ## If required define your own criterion\n",
    "# criterion = nn.BCELoss() ## If required define your own criterion\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_custom.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.2198, grad_fn=<DivBackward1>)\n",
      "tensor(6.4206, grad_fn=<DivBackward1>)\n",
      "tensor(6.9298, grad_fn=<DivBackward1>)\n",
      "tensor(8.4136, grad_fn=<DivBackward1>)\n",
      "tensor(11.1243, grad_fn=<DivBackward1>)\n",
      "tensor(6.2315, grad_fn=<DivBackward1>)\n",
      "tensor(6.1549, grad_fn=<DivBackward1>)\n",
      "tensor(6.6992, grad_fn=<DivBackward1>)\n",
      "tensor(4.6314, grad_fn=<DivBackward1>)\n",
      "tensor(6.4963, grad_fn=<DivBackward1>)\n",
      "tensor(7.4857, grad_fn=<DivBackward1>)\n",
      "tensor(7.3287, grad_fn=<DivBackward1>)\n",
      "tensor(5.6075, grad_fn=<DivBackward1>)\n",
      "tensor(5.0505, grad_fn=<DivBackward1>)\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 2\n",
    "batch_size = 16\n",
    "Y = torch.tensor(flat_dataset[\"label\"])\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for i in range(0 , len(X_img) , batch_size):\n",
    "\n",
    "        data_img = X_img[i:i + batch_size]\n",
    "        data_img = torch.from_numpy(data_img)\n",
    "        data_text = X_text[i:i + batch_size]\n",
    "        # targets = np.array([[0],[1]])\n",
    "        targets = Y[i : i + batch_size]\n",
    "        targets=targets.to(torch.float)\n",
    "        # data = data.cuda()\n",
    "        # targets = targets.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()   \n",
    "        # encoding = tokenizer.batch_encode_plus(data, return_tensors='pt', padding=True, truncation=True,max_length=50, add_special_tokens = True)\n",
    "        pixel_values = vit_processor(data_img , return_tensors=\"pt\")[\"pixel_values\"]\n",
    "        bertop = bert_processor(data_text.tolist(), padding=True, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids , attention_mask = bertop[\"input_ids\"] , bertop[\"attention_mask\"]\n",
    "        outputs = model_custom(pixel_values , input_ids , attention_mask)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6383]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ip1 = processor(X[0] , return_tensors=\"pt\")\n",
    "logits = model_custom(**ip1)\n",
    "pred_probab = nn.Sigmoid()(logits)\n",
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing PyTorch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "loss: 4.144501 \n",
      "loss: 3.394263 \n",
      "loss: 2.881017 \n",
      "loss: 5.493414 \n",
      "loss: 2.792670 \n",
      "loss: 4.001240 \n",
      "loss: 3.337624 \n",
      "loss: 2.815248 \n",
      "loss: 5.356466 \n",
      "loss: 2.645538 \n"
     ]
    }
   ],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(ClassificationHead, self).__init__()\n",
    "        # self.fc1 = nn.Linear(input_size, 128)\n",
    "        # self.fc2 = nn.Linear(128, 64)\n",
    "        # self.fc3 = nn.Linear(64, num_classes)\n",
    "        # self.relu = nn.ReLU()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(input_size, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, num_classes),\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x = self.fc1(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.fc2(x)\n",
    "        # x = self.relu(x)\n",
    "        # x = self.fc3(x)\n",
    "        # # x = self.sigmoid(x)\n",
    "        # return x\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "# Define your number of classes\n",
    "num_classes = NUM_CLASSES  # Number of classes in your classification task\n",
    "\n",
    "classification_head = ClassificationHead(vit_model.config.hidden_size, num_classes)\n",
    "\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss() ## If required define your own criterion\n",
    "# optimizer = torch.optim.Adam(vit_model.parameters(), lr=0.0001)\n",
    "optimizer = torch.optim.Adam(classification_head.parameters(), lr=0.0001)\n",
    "\n",
    "# Create the classification head\n",
    "\n",
    "EPOCHS = 2\n",
    "batch_size = 2\n",
    "Y = torch.tensor(flat_dataset[\"label\"])\n",
    "\n",
    "print(\"Training:\")\n",
    "# Training Loop\n",
    "running_loss = 0.0\n",
    "batch = 0\n",
    "for epoch in range(EPOCHS):\n",
    "    classification_head.train()\n",
    "    for i in range(0 , len(X_img) , batch_size):\n",
    "        batch += 1\n",
    "        inputs = X_img[i:i+batch_size]\n",
    "        labels = Y[i:i+batch_size]\n",
    "        labels=labels.to(torch.float)\n",
    "        \n",
    "        ip = vit_processor(inputs , return_tensors=\"pt\")\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = vit_model(**ip).last_hidden_state[:, 0, :]\n",
    "        logits = classification_head(outputs)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = loss_fn(logits, labels)\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # print(f\"EPOCH : {epoch} | BATCH : {i} | LOSS : {running_loss}\")\n",
    "        # print(loss)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss, current = loss.item(), batch * batch_size + len(inputs)\n",
    "        print(f\"loss: {loss:>7f} \")\n",
    "\n",
    "# print(\"Inference:\")\n",
    "# # Inference\n",
    "# with torch.no_grad():\n",
    "#     inputs = X\n",
    "#     ip = processor(inputs , return_tensors=\"pt\")\n",
    "#     outputs = model(**ip).last_hidden_state[:, 0, :]\n",
    "#     logits = classification_head(outputs)\n",
    "#     ans = nn.functional.sigmoid(logits)\n",
    "#     print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference:\n",
      "tensor([[0.0591, 0.0525, 0.0507, 0.0482, 0.0511, 0.0474, 0.0651, 0.0681, 0.0618,\n",
      "         0.0517, 0.0544, 0.0456, 0.0711, 0.0489, 0.0517, 0.0619, 0.0478, 0.0628],\n",
      "        [0.0591, 0.0525, 0.0507, 0.0482, 0.0511, 0.0474, 0.0651, 0.0681, 0.0618,\n",
      "         0.0517, 0.0544, 0.0456, 0.0711, 0.0489, 0.0517, 0.0619, 0.0478, 0.0628],\n",
      "        [0.0591, 0.0525, 0.0507, 0.0482, 0.0511, 0.0474, 0.0651, 0.0681, 0.0618,\n",
      "         0.0517, 0.0544, 0.0456, 0.0711, 0.0489, 0.0517, 0.0619, 0.0478, 0.0628],\n",
      "        [0.0591, 0.0525, 0.0507, 0.0482, 0.0511, 0.0474, 0.0651, 0.0681, 0.0618,\n",
      "         0.0517, 0.0544, 0.0456, 0.0711, 0.0489, 0.0517, 0.0619, 0.0478, 0.0628],\n",
      "        [0.0494, 0.0695, 0.0510, 0.0574, 0.0531, 0.0528, 0.0647, 0.0473, 0.0524,\n",
      "         0.0598, 0.0624, 0.0550, 0.0618, 0.0528, 0.0587, 0.0551, 0.0496, 0.0472],\n",
      "        [0.0494, 0.0695, 0.0510, 0.0574, 0.0531, 0.0528, 0.0647, 0.0473, 0.0524,\n",
      "         0.0598, 0.0624, 0.0550, 0.0618, 0.0528, 0.0587, 0.0551, 0.0496, 0.0472],\n",
      "        [0.0494, 0.0695, 0.0510, 0.0574, 0.0531, 0.0528, 0.0647, 0.0473, 0.0524,\n",
      "         0.0598, 0.0624, 0.0550, 0.0618, 0.0528, 0.0587, 0.0551, 0.0496, 0.0472],\n",
      "        [0.0477, 0.0560, 0.0474, 0.0447, 0.0494, 0.0547, 0.0643, 0.0473, 0.0648,\n",
      "         0.0648, 0.0609, 0.0538, 0.0626, 0.0605, 0.0657, 0.0611, 0.0480, 0.0464],\n",
      "        [0.0477, 0.0560, 0.0474, 0.0447, 0.0494, 0.0547, 0.0643, 0.0473, 0.0648,\n",
      "         0.0648, 0.0609, 0.0538, 0.0626, 0.0605, 0.0657, 0.0611, 0.0480, 0.0464],\n",
      "        [0.0477, 0.0560, 0.0474, 0.0447, 0.0494, 0.0547, 0.0643, 0.0473, 0.0648,\n",
      "         0.0648, 0.0609, 0.0538, 0.0626, 0.0605, 0.0657, 0.0611, 0.0480, 0.0464]])\n"
     ]
    }
   ],
   "source": [
    "print(\"Inference:\")\n",
    "# Inference\n",
    "with torch.no_grad():\n",
    "    inputs = X_img\n",
    "    ip = vit_processor(inputs , return_tensors=\"pt\")\n",
    "    outputs = vit_model(**ip).last_hidden_state[:, 0, :]\n",
    "    logits = classification_head(outputs)\n",
    "    logits = nn.Softmax(dim=1)(logits)\n",
    "    print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Training:\n",
    "tensor(4.2284, grad_fn=<DivBackward1>)\n",
    "tensor(3.3829, grad_fn=<DivBackward1>)\n",
    "tensor(2.9335, grad_fn=<DivBackward1>)\n",
    "tensor(5.4293, grad_fn=<DivBackward1>)\n",
    "tensor(2.7480, grad_fn=<DivBackward1>)\n",
    "tensor(4.1977, grad_fn=<DivBackward1>)\n",
    "tensor(3.3491, grad_fn=<DivBackward1>)\n",
    "tensor(2.8715, grad_fn=<DivBackward1>)\n",
    "tensor(5.2210, grad_fn=<DivBackward1>)\n",
    "tensor(2.5404, grad_fn=<DivBackward1>)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TensorFlow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "tokenized_data = tokenizer(dataset[\"question\"], return_tensors=\"np\", padding=True)\n",
    "# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels = Y.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': array([[ 101, 1327, 1110, ...,    0,    0,    0],\n",
       "        [ 101, 1327, 1700, ...,    0,    0,    0],\n",
       "        [ 101, 1327, 2942, ...,    0,    0,    0],\n",
       "        ...,\n",
       "        [ 101, 1731, 1242, ...,    0,    0,    0],\n",
       "        [ 101, 2181, 1175, ...,    0,    0,    0],\n",
       "        [ 101, 1731, 1242, ...,    0,    0,    0]]),\n",
       " 'token_type_ids': array([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]),\n",
       " 'attention_mask': array([[1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0],\n",
       "        [1, 1, 1, ..., 0, 0, 0]])}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;28;01mNone\u001b[39;00m,))\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2858\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.__call__\u001b[0;34m(self, text, text_pair, text_target, text_pair_target, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2856\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_in_target_context_manager:\n\u001b[1;32m   2857\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_input_mode()\n\u001b[0;32m-> 2858\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_one\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2859\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2860\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_switch_to_target_mode()\n",
      "File \u001b[0;32m~/miniconda3/envs/python38/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2916\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase._call_one\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2913\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   2915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text):\n\u001b[0;32m-> 2916\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2917\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2918\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2919\u001b[0m     )\n\u001b[1;32m   2921\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_valid_text_input(text_pair):\n\u001b[1;32m   2922\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2923\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2924\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mor `List[List[str]]` (batch of pretokenized examples).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2925\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: text input must be of type `str` (single example), `List[str]` (batch or single pretokenized example) or `List[List[str]]` (batch of pretokenized examples)."
     ]
    }
   ],
   "source": [
    "x = tf.keras.Input(shape=(None,))\n",
    "tokenizer(x, return_tensors=\"np\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda3/envs/python38/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "All PyTorch model weights were used when initializing TFViTForImageClassification.\n",
      "\n",
      "All the weights of TFViTForImageClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFViTForImageClassification for predictions without further training.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'patch_embeddings' (type TFViTPatchEmbeddings).\n\nMake sure that the channel dimension of the pixel values match with the one set in the configuration.\n\nCall arguments received by layer 'patch_embeddings' (type TFViTPatchEmbeddings):\n  • pixel_values=tf.Tensor(shape=(100, 480, 640, 3), dtype=uint8)\n  • interpolate_pos_encoding=None\n  • training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[82], line 26\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# # Load and compile our model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# # tf_model = tf.keras.models.Sequential(\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# #     [\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# tf_model.fit(X, labels)\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# # tf_model\u001b[39;00m\n\u001b[1;32m     25\u001b[0m tf_model \u001b[38;5;241m=\u001b[39m TFAutoModelForImageClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/vit-base-patch16-224\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m \u001b[43mtf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python38/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/miniconda3/envs/python38/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python38/lib/python3.8/site-packages/transformers/models/vit/modeling_tf_vit.py:872\u001b[0m, in \u001b[0;36mTFViTForImageClassification.call\u001b[0;34m(self, pixel_values, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict, labels, training)\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(VIT_INPUTS_DOCSTRING)\n\u001b[1;32m    848\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    863\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    864\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFSequenceClassifierOutput, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m    865\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    866\u001b[0m \u001b[38;5;124;03m    labels (`tf.Tensor` or `np.ndarray` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;124;03m        Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;124;03m        config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;124;03m        `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m    870\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 872\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    881\u001b[0m     sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    882\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(inputs\u001b[38;5;241m=\u001b[39msequence_output[:, \u001b[38;5;241m0\u001b[39m, :])\n",
      "File \u001b[0;32m~/miniconda3/envs/python38/lib/python3.8/site-packages/transformers/modeling_tf_utils.py:437\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    436\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 437\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/python38/lib/python3.8/site-packages/transformers/models/vit/modeling_tf_vit.py:596\u001b[0m, in \u001b[0;36mTFViTMainLayer.call\u001b[0;34m(self, pixel_values, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict, training)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pixel_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    594\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou have to specify pixel_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 596\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m head_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/python38/lib/python3.8/site-packages/transformers/models/vit/modeling_tf_vit.py:130\u001b[0m, in \u001b[0;36mTFViTEmbeddings.call\u001b[0;34m(self, pixel_values, interpolate_pos_encoding, training)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;28mself\u001b[39m, pixel_values: tf\u001b[38;5;241m.\u001b[39mTensor, interpolate_pos_encoding: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, training: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    128\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tf\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    129\u001b[0m     batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m shape_list(pixel_values)\n\u001b[0;32m--> 130\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpatch_embeddings\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterpolate_pos_encoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;66;03m# add the [CLS] token to the embedded patch tokens\u001b[39;00m\n\u001b[1;32m    135\u001b[0m     cls_tokens \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcls_token, repeats\u001b[38;5;241m=\u001b[39mbatch_size, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/python38/lib/python3.8/site-packages/transformers/models/vit/modeling_tf_vit.py:189\u001b[0m, in \u001b[0;36mTFViTPatchEmbeddings.call\u001b[0;34m(self, pixel_values, interpolate_pos_encoding, training)\u001b[0m\n\u001b[1;32m    187\u001b[0m batch_size, num_channels, height, width \u001b[38;5;241m=\u001b[39m shape_list(pixel_values)\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly() \u001b[38;5;129;01mand\u001b[39;00m num_channels \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_channels:\n\u001b[0;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMake sure that the channel dimension of the pixel values match with the one set in the configuration.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    191\u001b[0m     )\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m interpolate_pos_encoding:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'patch_embeddings' (type TFViTPatchEmbeddings).\n\nMake sure that the channel dimension of the pixel values match with the one set in the configuration.\n\nCall arguments received by layer 'patch_embeddings' (type TFViTPatchEmbeddings):\n  • pixel_values=tf.Tensor(shape=(100, 480, 640, 3), dtype=uint8)\n  • interpolate_pos_encoding=None\n  • training=False"
     ]
    }
   ],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification , TFAutoModel , TFAutoModelForImageClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load and compile our model\n",
    "# tf_model = tf.keras.models.Sequential(\n",
    "#     [\n",
    "#         TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\"),\n",
    "#         tf.keras.layers.Dense(1 , input_shape=(768,) , activation='sigmoid')\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# x = tf.keras.Input(shape=)\n",
    "# tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "proc = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "tf_model = TFAutoModel.from_pretrained(\"google/vit-base-patch16-224\")(**tf_model)\n",
    "# tf_model = tf.keras.layers.Dense(1 , input_shape=(768,1) , activation='sigmoid')(tf_model.logits)\n",
    "\n",
    "# # Lower learning rates are often better for fine-tuning transformers\n",
    "# tf_model.compile(optimizer=Adam(3e-5))  # No loss argument!\n",
    "\n",
    "tf_model.fit(X, labels)\n",
    "# tf_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
